{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando CVs de integrantes do Ministério das Mulheres, Família e Direitos Humanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código adapta uma ferramenta de análise de cúrriculos na área de recursos humanos. Assim, o modelo extrai textos de currículos de integrantes do MMFDH. A partir de termos encontrados no interior dos textos dos currículos, busco CVs que se enquadram num perfil específico [egressosos da Mackenzie, da Fundação Republicana Brasileira, formados em teologia e que utilizam a palavra \"deus\" em seus textos, entre outras informações], e que reflete um perfil de pessoas provenientes do campo religioso. As wordclouds nos auxilian na indentificação dessa tendencia, uma vez que a repetição de palavras e termos podem nos mostrar o perfil dos atores, que corresponde a uma análise qualitativa auxiliar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intalar o pacote pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import glob \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diretório onde estão salvos os PDF´s dos CVs do MMFDH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é importante que insira o seu diretório aqui\n",
    "e = r'C:\\Users\\Andre G\\Documents\\Pesquisa\\Scripts Webscraping\\Modelo (Currículos)\\curriculos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter os endereços com os nomes dos arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntar o diretório onde está o arquivo com o nome dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endereco = []\n",
    "for i in os.listdir('./curriculos/'): # você poderá incluir o diretório, caso seja necessário.\n",
    "    local = e + \"\\\\\" + i\n",
    "    endereco.append(local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de currículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(endereco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para extrair e juntar o texto de várias páginas de um PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_pdf(pdf_path):\n",
    "    all_text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for pdf_page in pdf.pages:\n",
    "            single_page_text = pdf_page.extract_text()\n",
    "            all_text = all_text + '\\n' + single_page_text\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva texto sujo dos currículos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texto = []\n",
    "curriculos = []\n",
    "\n",
    "for arquivo in tqdm(endereco):\n",
    "    try:\n",
    "        nome = arquivo.split('\\\\')[-1].replace('.pdf','').replace('.docx','')\n",
    "        #print(nome)\n",
    "        te = extract_pdf(arquivo).split()\n",
    "        low = [palavra.lower() for palavra in te]\n",
    "        tex = \" \".join(s for s in low)\n",
    "        texto.append([nome,tex])\n",
    "        curriculos = pd.DataFrame(texto,columns=['Nome', 'Texto'] )\n",
    "        curriculos.to_csv('texto_completo_curriculos.csv', sep=';', encoding=\"utf16\")\n",
    "    except Exception:\n",
    "        pass\n",
    "len(curriculos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva texto limpo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv_limpo = []\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "for arquivo in tqdm(endereco):\n",
    "    try:\n",
    "        nome = arquivo.split('\\\\')[-1].replace('.pdf','').replace('.docx','')\n",
    "        tex = extract_pdf(arquivo)\n",
    "\n",
    "        lista_de_palavras = nltk.tokenize.word_tokenize(tex)\n",
    "        lista_de_palavras = [palavra.lower() for palavra in lista_de_palavras]\n",
    "\n",
    "        stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "        pontuação =['(',')',':','[',']',',','|',';',',','.', '_']\n",
    "\n",
    "        keywords = [palavra for palavra in lista_de_palavras if not palavra in stop_words and not palavra in pontuação]\n",
    "\n",
    "        textocv = \" \".join(s for s in keywords)\n",
    "\n",
    "        cv_limpo.append([nome,textocv])\n",
    "        \n",
    "        cvlimpo = pd.DataFrame(cv_limpo,columns=['Nome', 'Texto_cv_limpo'])\n",
    "        cvlimpo.to_csv('texto_cv_limpo.csv', sep=';', encoding=\"utf16\")\n",
    "    except Exception:\n",
    "            pass\n",
    "len(cv_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para salvar currículos e wordcloud: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) wordcloud´s de cada currículo;                             \n",
    "2) aquivo csv com os textos dos currículos sem limpeza;                                     \n",
    "3) aquivo csv com os textos dos currículos sem pontuação e sem stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(cv, salvar=True):\n",
    "    '''\n",
    "    cv: diretório do arquivo PDF\n",
    "    '''\n",
    "    cv_limpo = []\n",
    "    texto = []\n",
    "    curriculos = []\n",
    "    \n",
    "    for arquivo in endereco:\n",
    "        try:\n",
    "            nome = arquivo.split('\\\\')[-1].replace('.pdf','').replace('.docx','')\n",
    "            print(nome)\n",
    "            tex = extract_pdf(arquivo)\n",
    "\n",
    "            # Salva CSV texto sujo\n",
    "            te = extract_pdf(arquivo).split()\n",
    "            low = [palavra.lower() for palavra in te]\n",
    "            tex = \" \".join(s for s in low)\n",
    "            texto.append([nome,tex])\n",
    "            curriculos = pd.DataFrame(texto,columns=['Nome', 'Texto'] )\n",
    "            curriculos.to_csv('texto_completo_curriculos.csv', sep=';', encoding='utf16')\n",
    "\n",
    "            lista_de_palavras = nltk.tokenize.word_tokenize(tex)\n",
    "            lista_de_palavras = [palavra.lower() for palavra in lista_de_palavras]\n",
    "\n",
    "            stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "            pontuação =['(',')',':','[',']',',','|',';',',','.','_']\n",
    "\n",
    "            keywords = [palavra for palavra in lista_de_palavras if not palavra in stop_words and not palavra in pontuação]\n",
    "\n",
    "            textocv = \" \".join(s for s in keywords)\n",
    "\n",
    "            #Salva CSV texto limpo\n",
    "            cv_limpo.append([nome,textocv])\n",
    "            cvlimpo = pd.DataFrame(cv_limpo,columns=['Nome', 'Texto_cv_limpo'] )\n",
    "            cvlimpo.to_csv('texto_cv_limpo.csv', sep=';', encoding='utf16')\n",
    "\n",
    "            wordcloud = WordCloud(background_color = '#0f54c9',\n",
    "                                 max_font_size = 150,\n",
    "                                 width = 1280,\n",
    "                                 height = 720, \n",
    "                                 colormap = \"Blues\",\n",
    "                                 max_words=50).generate(textocv)\n",
    "            fig, ax = plt.subplots(figsize=(16,9))\n",
    "            ax.imshow(wordcloud)\n",
    "            ax.set_axis_off()\n",
    "            plt.imshow(wordcloud)\n",
    "            if salvar:\n",
    "                wordcloud.to_file(nome + '_wordcloud.png')\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantidade de textos de currículos \"sujos\" e \"limpos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(curriculos),len(cv_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamando a função que salva os bancos e também salva as nuvens de palvras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(endereco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dando match em CVs do MMFDH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vagas = pd.read_excel('dados_formação.xlsx', sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vagas = len(vagas.keys())\n",
    "nome_vagas = list(vagas.keys())\n",
    "n_vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vagas = [vagas[nome_vagas[i]] for i in range(n_vagas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perfil alvo\n",
    "vagas[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os termos da busca\n",
    "palavras_chaves = list(vagas[4]['palavras-chave'])\n",
    "palavras_chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando a lista de dados biograficos\n",
    "lista_de_vagas = vagas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de entrada: CVs saida por vaga segundo score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontuacao = ['(', ')', ';', ':', '[', ']', ',']\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "cv = list(endereco)\n",
    "\n",
    "def MatchCV(cv, vaga, limite = 5):\n",
    "    '''\n",
    "    cv: caminho de um arquivo PDF\n",
    "    vaga: dataset de palavras-chave e pesos\n",
    "    '''\n",
    "    \n",
    "    for arquivo in cv:\n",
    "        nome = cv.split('\\\\')[-1].replace('.pdf','').replace('.docx','')\n",
    "        tex = extract_pdf(cv)\n",
    "\n",
    "        lista_de_palavras = nltk.tokenize.word_tokenize(tex)\n",
    "        lista_de_palavras = [palavra.lower() for palavra in lista_de_palavras]\n",
    "\n",
    "        keywords = [palavra for palavra in lista_de_palavras if not palavra in stop_words and not palavra in pontuacao]\n",
    "\n",
    "        textocv = \" \".join(s for s in keywords)\n",
    "\n",
    "        pesos = list(vaga['pesos'])\n",
    "        palavras_chaves = list(vaga['palavras-chave'])\n",
    "\n",
    "        cont = [textocv.count(pc) for pc in palavras_chaves]  # conta quantas vezes cada termo da vaga aparece no texto do cv\n",
    "\n",
    "        def aux(x, limite):\n",
    "            return x if x <= limite else limite\n",
    "\n",
    "        cont = [aux(i, limite) for i in cont]   # coloca o limite na contagem de palavras\n",
    "\n",
    "        pmax = np.sum(np.array(pesos) * limite) \n",
    "\n",
    "        score = ((np.array(cont) * pesos).sum()/pmax).round(4)\n",
    "\n",
    "        return score\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista de Listas: cada lista interna mostra o score de uma pessoa em relação ao perfil alvo do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pessoas = [[MatchCV(cvs, vaga) for vaga in lista_de_vagas] for cvs in cv]\n",
    "pessoas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = [arquivo.split('\\\\')[-1].replace('.pdf','').replace('.docx','') for arquivo in endereco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de dados comos matches das pessoas com as vagas\n",
    "matchs = pd.DataFrame(pessoas, columns = nome_vagas, index = nomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_vagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = matchs.sort_values(by = 'curriculos_religiosos', ascending = False)\n",
    "match.to_excel('matchs.xlsx', encoding=\"CP1252\")\n",
    "match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tem formação superior ou maior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(match.formação.value_counts(normalize=True,ascending=True)*100).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É servidor de carreira?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(match.servidor_carreira.value_counts(normalize=True,ascending=True)*100).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfil alvo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match[match['curriculos_religiosos']>0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de currículos com perfil religioso\n",
    "len(match[match['curriculos_religiosos']>0.0001])/133*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
